cross validation não é  a melhor opção para fazer a avaliação dos modelos --> prof disse nas teoricas q n é mesmo para usar

usar nesta fase apenas a tabela com a coluna target.

a é criar para já um pipeline de 
    limpeza --> training --> evaluation

assim podemos testar varios casos de limpeza de DF com diferentes modelos e testar assim muitos algoritmos com diferentes estrategias de limpeza.


o pipeline serve para rapidamente nos indicar se a qualidade do modelo aumenta ou diminui face aos inputs da pipeline.

como dar o score da qualidade?
    --> vai variando pq a vida é complicada e os modelos são estilos diferentes 

o prof sugeriu fazer treino 1-4 testar 5    |    treinar 1-5 testar 6 ... --> growing window. ouuuuu sliding window em que usamos apenas os 3 anos anteriores ao ano que queremos prever por ex. pq com o passar dos anos, anos mais antigos perdem relevancia ig --> testar ambos
