{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autosklearn.classification\n",
    "from pipeline import *\n",
    "\n",
    "# pipeline_year(10)\n",
    "\n",
    "df_coaches = pd.read_csv('dataset/cleaned/coaches.csv')\n",
    "l = []\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "decay_rate=0.1\n",
    "df_teams_merged = []\n",
    "test = []\n",
    "train = []\n",
    "year = 10\n",
    "for i in range(2, year + 1):\n",
    "    df_teams_merged = pipeline_clf(year=i)\n",
    "    weight = decay_rate ** (10 - i - 1)\n",
    "\n",
    "    df_teams_merged['confID'] = df_teams_merged['confID'].replace({'EA': 0, 'WE': 1})\n",
    "\n",
    "    train = df_teams_merged[df_teams_merged['year'] < i]\n",
    "    test = df_teams_merged[df_teams_merged['year'] == i]\n",
    "\n",
    "    if (year == i):\n",
    "        break\n",
    "\n",
    "    X_train = train[train.drop(['playoff', 'year', 'tmID'], axis=1).columns]\n",
    "    y_train = train['playoff']\n",
    "    sample_weight = [weight] * len(X_train)\n",
    "\n",
    "    clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "predictions = clf.predict_proba(test.drop(['playoff', 'year', 'tmID'], axis=1))[:, 1]\n",
    "test['predictions'] = predictions\n",
    "df_teams_merged['predictions'] = 0\n",
    "df_teams_merged.loc[df_teams_merged['year'] == year, 'predictions'] = predictions\n",
    "\n",
    "df_teams_merged['confID'] = df_teams_merged['confID'].replace({0: 'EA', 1 : 'WE'})\n",
    "\n",
    "# print the year and the predicted scores\n",
    "print(df_teams_merged[df_teams_merged['year'] == year][['tmID', 'confID', 'predictions', 'awards', 'offensive_strength']].sort_values(by='predictions', ascending=False))\n",
    "\n",
    "\n",
    "df_teams, ea_teams, we_teams = classify_playoff_entry(\n",
    "        df_teams_merged, year)\n",
    "\n",
    "ea_predictions = ea_teams['tmID'].unique()\n",
    "we_predictions = we_teams['tmID'].unique()\n",
    "\n",
    "\n",
    "accuracy = calculate_playoff_accuracy(\n",
    "    year, ea_predictions, we_predictions, display_results = True)\n",
    "\n",
    "print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_year(8,display_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import *\n",
    "\n",
    "def check_accuracy_by_year2():\n",
    "    accs = []\n",
    "    years = list(range(2, 11))\n",
    "\n",
    "    for year in years:\n",
    "        acc = pipeline_year(year)\n",
    "        accs.append(acc)\n",
    "\n",
    "    # plot the accuracy line graph\n",
    "    plt.plot(years, accs, label=\"Accuracy\", marker='o', linestyle='-')\n",
    "\n",
    "    # add labels for each data point\n",
    "    # for i, acc in enumerate(accs):\n",
    "      #  plt.text(years[i], acc, f\"{acc:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # add legend\n",
    "    plt.legend()\n",
    "\n",
    "    # set Y-axis limits\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy by year\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "check_accuracy_by_year2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
